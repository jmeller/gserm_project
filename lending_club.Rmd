---
title: 'First Draft Lending Club Model'
output:
  html_document:
    df_print: paged
---

# Data loading
```{r data_loading, results = "hide"}
library(data.table)
library(randomForest)
library(ranger)
library(magrittr)
library(caret)
library(dplyr)
library(tidyverse)

# features not to use
# taboo.list <- c("last_fico_range_high", "last_fico_range_low", "acc_now_delinq")
taboo.list <- c("")

train.raw <- fread("lending_club_train.csv", stringsAsFactors = T)
test.raw <- fread("lending_club_test.csv", stringsAsFactors = T)
```

# Data preprocessing
```{r data_preprocessing}
# combine train and test data for data handling
data.raw <- union(
  # the mutate function adds a source so we can split it again later
  train.raw[, data_source := "train"],
  test.raw[, `:=`(data_source = "test",
                  default = as.integer(default))]
) 

# confirm that the union worked correctly
table(data.raw$data_source)

# clean/transform features
data.raw %<>% .[,`:=`(title = factor(title),
          zip_code = factor(zip_code),
          revol_util = as.numeric(str_replace_all(revol_util, "[%]", "")),
          default = factor(default, 
                           levels = c("0","1"), 
                           labels = c("no_default", "default")))]

# clean memory
rm(train.raw, test.raw)
```
# Missing value handling
```{r missing value handling}
# divide data set into subsets w. numeric and categorical data
col.classes <- sapply(data.raw, class)
num.features <- which(col.classes %in% c("numeric", "integer"))
cat.features <- c(1, which(col.classes %in% c("factor", "logical", "character")))
data.num.features <- data.raw[,num.features, with = F]
data.cat.features <- data.raw[,cat.features, with = F]

# cat features: replace NA w. "Missing"
fill.rates.cat <- data.cat.features[,lapply(.SD, 
                               function(col) sum(1 - as.integer(is.na(col)))/nrow(data.cat.features)), 
                       .SDcols = names(data.cat.features)] %>% unlist
## remove cols w.o. any data
exclude.cat <- fill.rates.cat[which(fill.rates.cat == 0)] 
data.cat.features %<>% .[,!names(exclude.cat), with = F]
data.cat.features %<>% 
  .[,lapply(.SD, function(col) ifelse(is.na(col), "Missing", col)), 
    .SDcols = c(1,3:ncol(.))]

# numeric features
fill.rates.num <- data.num.features[,lapply(.SD, 
                               function(col) sum(1 - as.integer(is.na(col)))/nrow(data.num.features)), 
                       .SDcols = names(data.num.features)] %>% unlist
## remove cols with > 75% missing data
cols.wNAs.num <- fill.rates.num[which(fill.rates.num > 0.75)]
data.wNAs.num <- data.num.features[, c(names(cols.wNAs.num)), with = F]

## impute NAs by means
impute.cols <- names(data.wNAs.num)[which(!names(data.wNAs.num) == "id")]
data.wNAs.num <- data.wNAs.num[,paste0("dmi_", impute.cols) := # add dmis
                           lapply(.SD, function(col) ifelse(is.na(col), 1, 0)),
                         .SDcols = impute.cols]
data.imputed.num <- data.wNAs.num[,(impute.cols) := 
                              lapply(.SD, function(col) 
                                ifelse(is.na(col), mean(col, na.rm = T), col)), 
                            .SDcols = impute.cols]
# merge back
target <- data.raw[,.(id, default)]
data.preprocessed <- merge(data.cat.features, data.imputed.num, by = "id") %>% merge(target, by = "id")
```

```{r feature engineering}
# feature engineering
data.preprocessed[,`:=`(emp_title = as.factor(ifelse(str_detect(tolower(emp_title), "manager|director|ceo"),
                                                 "executive",
                                                 ifelse(str_detect(tolower(emp_title), "engineer|specialist"),
                                                        "specialist",
                                                        "other"))),
                        last_credit_pull_d_month = as.factor(str_sub(last_credit_pull_d, 1,3)),
                        last_credit_pull_d_year = as.factor(str_sub(last_credit_pull_d, 5,8)),
                        last_credit_pull_d = NULL,
                        earliest_cr_line_month = as.factor(str_sub(earliest_cr_line, 1,3)),
                        earliest_cr_line_year = str_sub(earliest_cr_line, 5,8),
                        earliest_cr_line = NULL,
                        zip_2 = as.factor(str_sub(zip_code, 1, 3)))] %>%
  .[,`:=`(length_bs_rel = (2018 - as.integer(earliest_cr_line_year)),
          earliest_cr_line_year = as.factor(earliest_cr_line_year))]
```



# Feature selection
```{r feature_selection}
# data.preprocessed <- data.raw
# manually exclude taboo features
data.preprocessed %<>% .[,!taboo.list, with = F] %>% .[order(id)]

train <- data.preprocessed[data_source == "train"] %>% .[,!which(names(.) %in% 
                                                                   c("id", "data_source")), 
                                                         with = F]

set.seed(2608)
# subset.train <- train[sample(1:nrow(train), 10000),
#                     which(names(train) %in% c("default", feat.manual)), with = F]
# feature importance based on downsampled training data
# downSample(train, train$default) %>% select(-Class) -> train.downsample
# subset.train <- train[sample(1:nrow(train), 20000), ]

fit <- ranger(default ~ ., train, importance = "impurity", num.trees = 20)
importance.feat <- importance(fit) %>% .[order(., decreasing = T)]
importance.feat

top30.feat <- names(importance.feat)[1:30]
top30.feat

# manual.feat <- c("acc_now_delinq", "delinq_amnt", "fico_range_low", 
#                  "num_tl_30dpd", "pct_tl_nvr_dlq", "revol_util")
# 
# final.feat <- union(top10.feat, manual.feat)
final.feat <- top30.feat
# clean up memory
rm(train.downsample)
```
# Model tuning and evaluation
```{r evaluation}

library(caret)
set.seed(2608)
# train.data_set
train <- data.preprocessed[data_source == "train"] %>% .[,!which(names(.) %in% 
                                                                   c("id", "data_source")), 
                                                         with = F] 
train.data.all <- train[, c("default", final.feat), with = F]
train.data.subset <- train.data.all[sample(1:nrow(train), 10000)] # larger training sets kill my memory...

ranger.grid <- expand.grid(mtry = c(4,5,6), splitrule = "gini", min.node.size = 1)

# create the caret experiment using the trainControl() function
ctrl <- trainControl(
  method = "cv", number = 10, # 10-fold CV
  selectionFunction = "best", # select the best performer
  classProbs = TRUE, # requested the predicted probs (for ROC)
  summaryFunction = twoClassSummary, # needed to produce the ROC/AUC measures
  savePredictions = TRUE # needed to plot the ROC curves
)

# train the decision tree model using 10-fold CV
m.ranger <- train(default ~ .,
                 data = train.data.subset,
                 method = "ranger",
                 metric = "ROC",
                 trControl = ctrl, 
                 tuneGrid = ranger.grid, verbose = T,
                 num.trees = 500)

m.ranger

# train xgb model
xgb.grid <- expand.grid(nrounds = c(100), 
                        max_depth = c(6), eta = 0.3, 
                        subsample = 1, colsample_bytree = 1, 
                        gamma = 0, min_child_weight = 1)
m.xgboost <- train(default ~.,
                   data = train.data.subset,
                   method = "xgbTree",
                   metric = "ROC",
                   tuneGrid = xgb.grid,
                   trControl = ctrl, verbose = T)

m.xgboost
```
```{r roc}
library(pROC)

# save the predicted probabilities and actual values for decision tree
predictions <- predict(m.ranger, train.data.subset, type = "prob")
roc.ranger <- roc(predictor = predictions$default, response = train.data.subset$default)


table(predictions$default > 0.5, train.data.subset$default)
auc.ranger <- round(auc(roc.ranger), 20)
auc.ranger
```

```{r predict}
test <- data.preprocessed[data_source == "test"] %>% .[,!which(names(.) == "data_source"), with = F]
test$P_default <- predict(m.ranger, test, type = "prob")$default

final.predictions <- test[,.(id, P_default)] %>% as.data.frame

write_csv(final.predictions, "6.csv")
```



```{r sandbox}

train.data <- data.raw %>% filter(data_source == "train") %>%
  select(c("default", "loan_amnt", "term", "home_ownership", "annual_inc", 
           "total_acc", "tot_cur_bal", "avg_cur_bal", "mort_acc", 
           "tot_hi_cred_lim", "total_bal_ex_mort", "total_il_high_credit_limit")) %>%
  mutate(default = factor(default, levels = c("0", "1"), labels = c("non_default", "default")),
         loan_amnt = as.numeric(loan_amnt),
         term = as.factor(term), 
         home_ownership = as.factor(home_ownership),
         annual_inc = as.numeric(annual_inc),
         total_acc = as.numeric(total_acc),
         tot_cur_bal = as.numeric(tot_cur_bal),
         avg_cur_bal = as.numeric(avg_cur_bal),
         mort_acc = as.factor(mort_acc),
         tot_hi_cred_lim = as.numeric(tot_hi_cred_lim),
         total_bal_ex_mort = as.numeric(total_bal_ex_mort),
         total_il_high_credit_limit = as.numeric(total_il_high_credit_limit))








data.raw %>% .[,lapply(.SD, anyNA)] %>% unlist %>% which %>% .[-1] -> select.cols
noNA.data <- data.raw[,!select.cols, with = F] %>% .[data_source ==  "train"]

library(rpart)
fit.rpart <- rpart(factor(default) ~ ., noNA.data)
fit.rpart$variable.importance %>% .[order(., decreasing = T)] %>% names-> imp.cols

# final.cols <- imp.cols[-which(imp.cols %in% c("id", "emp_title", "zip_code", "earliest_cr_line", "desc"))]
final.cols <- imp.cols[-which(imp.cols %in% c("id", "desc"))]

train.data <- data.raw[data_source == "train",] %>% .[,default := factor(default, levels = c("0", "1"), labels = c("non_default", "default"))]
train.data.subset <- train.data[sample(1:nrow(train.data), 25000), c("default", "loan_amnt", "term", "home_ownership", "annual_inc", "total_acc", "tot_cur_bal", "avg_cur_bal", "mort_acc", "tot_hi_cred_lim", "total_bal_ex_mort", "total_il_high_credit_limit"), with = F] # larger training sets kill my memory...

ranger.grid <- expand.grid(mtry = 4, splitrule = "gini", min.node.size = 1)

# create the caret experiment using the trainControl() function
ctrl <- trainControl(
  method = "cv", number = 5, # 10-fold CV
  selectionFunction = "best", # select the best performer
  classProbs = TRUE, # requested the predicted probs (for ROC)
  summaryFunction = twoClassSummary, # needed to produce the ROC/AUC measures
  savePredictions = TRUE # needed to plot the ROC curves
)

# train the decision tree model using 10-fold CV
m.rpart <- train(factor(default) ~ .,
                 data = train.data.subset,
                 method = "ranger",
                 metric = "ROC",
                 trControl = ctrl,
                 num.trees = 100, 
                 tuneGrid = ranger.grid,
                 verbose = T,
                 importance = "impurity")

m.rpart
varImp(m.rpart)


train.raw %<>% mutate(default = factor(default, levels = c("0", "1"), labels = c("non_default", "default")))
train.data.set <- train.raw %>% select(c("default", "loan_amnt", "term", "home_ownership", "annual_inc", "total_acc", "tot_cur_bal", "avg_cur_bal", "mort_acc", "tot_hi_cred_lim", "total_bal_ex_mort", "total_il_high_credit_limit"))
set.seed(123)
train.data.set <- train.data[sample(1:nrow(train.data), 5000),]

ranger.grid <- expand.grid(mtry = 4)

# create the caret experiment using the trainControl() function
ctrl <- trainControl(
  method = "cv", number = 5, # 10-fold CV
  selectionFunction = "best", # select the best performer
  classProbs = TRUE, # requested the predicted probs (for ROC)
  summaryFunction = twoClassSummary, # needed to produce the ROC/AUC measures
  savePredictions = TRUE # needed to plot the ROC curves
)

# train the decision tree model using 10-fold CV
m.ranger <- train(default ~ .,
                 data = train.data.set,
                 method = "rf",
                 metric = "ROC",
                 trControl = ctrl,
                 tuneGrid = ranger.grid,
                 verbose = T)

m.ranger
varImp(m.ranger)
```

